{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  4 09:52:24 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.51.01              Driver Version: 532.03       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:09:00.0  On |                  N/A |\n",
      "| 97%   45C    P2              147W / 350W|  12885MiB / 24576MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        22      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        26      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "class Net(nn.Sequential):\n",
    "    def __init__(self, i):\n",
    "        out_size = [246016, 57600, 12544, 2304, 256]\n",
    "        super().__init__(\n",
    "            nn.Conv2d(3, 32, 3, 1), nn.ReLU(),\n",
    "            *[layer for _ in range(i) for layer in [nn.Conv2d(32, 32, 3, 1), nn.MaxPool2d(2), nn.Dropout(0.25)]],\n",
    "            nn.Conv2d(32, 64, 3, 1), nn.MaxPool2d(2), nn.Dropout(0.25),\n",
    "            Flatten(), nn.Linear(out_size[i], 128), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(128, 2), nn.LogSoftmax(dim=1) )\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx*len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx/len(train_loader), loss.item()))\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses)/len(losses)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Store predictions and targets for recall calculation\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    recalls = recall_score(all_targets, all_preds, average=None)  # compute recall for each class\n",
    "    class_recall = recalls[1]  # replace 0 with the index of the class you're interested in\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Recall: {:.2f}\\n'.format(\n",
    "    #     test_loss/len(test_loader.dataset), class_recall))\n",
    "\n",
    "    return class_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        path_img, label = self.data_info.iloc[item][1:3]\n",
    "        label = int(label)\n",
    "        path_img = os.path.join(self.data_dir, path_img)\n",
    "        image = Image.open(path_img).convert('RGB') # Gray scale is enough for logic interpretation\n",
    "        # 使用定义好的transforms，对数据进行处理\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    def get_img_info(self, data_dir):\n",
    "        path_dir = os.path.join(data_dir, 'label.csv')\n",
    "        return pd.read_csv(path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folders = []\n",
    "for i in range(1, 5):\n",
    "    for j in range(1, 5):\n",
    "        if i < j:\n",
    "            continue\n",
    "        elif i == j:\n",
    "            dataset_folders.append(f\"dataset0{i}\")\n",
    "        else:\n",
    "            dataset_folders.append(f\"dataset0{i}x0{j}\")\n",
    "\n",
    "exp_name = \"dataset_cross_validation_study\"\n",
    "\n",
    "if not os.path.exists(f'./models/{exp_name}'):\n",
    "    os.mkdir(f'./models/{exp_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:12<06:19,  6.32s/it]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "kwargs = {'num_workers': 16}\n",
    "batch_size=128\n",
    "epochs, lr = 100, 1e-4\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((128, 128)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "for dataset_folder in dataset_folders:\n",
    "    trainset = MyDataset(f\"data/train/{dataset_folder}\", transform)\n",
    "    testset = MyDataset(f\"data/test/{dataset_folder}\", transform)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Loss', 'Recall'])\n",
    "    \n",
    "    model_name = f\"1-net-{dataset_folder}\"\n",
    "\n",
    "    model = Net(1).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    model_path = f'./models/{exp_name}/{model_name}.pt'\n",
    "    history_path = f'./models/{exp_name}/{model_name}_history.csv'\n",
    "\n",
    "    if os.path.exists(history_path):\n",
    "        df = pd.read_csv(history_path)\n",
    "    else:\n",
    "        previous_best_recall = -1\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            loss = train(model, device, train_loader, optimizer, epoch)\n",
    "            recall = test(model, device, test_loader)\n",
    "            \n",
    "            df.loc[len(df)] = [loss, recall]\n",
    "            \n",
    "            if recall > previous_best_recall:\n",
    "                previous_best_recall = recall\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "        df.to_csv(history_path, index=False)\n",
    "\n",
    "    # Assuming df is your DataFrame and it has columns 'Loss' and 'Class 0 Recall'\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot Loss\n",
    "    sns.lineplot(data=df, x=df.index, y='Loss', ax=ax1, color='blue')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis and plot Recall on it\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=df, x=df.index, y='Recall', ax=ax2, color='red')\n",
    "    ax2.set_ylabel('Recall', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    ax1.set_xlabel('Epochs')\n",
    "\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-net-InDL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.32      0.45       282\n",
      "           1       0.33      0.79      0.46       118\n",
      "\n",
      "    accuracy                           0.46       400\n",
      "   macro avg       0.55      0.55      0.46       400\n",
      "weighted avg       0.65      0.46      0.46       400\n",
      "\n",
      "1-net-InDL\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       282\n",
      "           1       0.00      0.00      0.00       118\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.35      0.50      0.41       400\n",
      "weighted avg       0.50      0.70      0.58       400\n",
      "\n",
      "2-net-InDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       282\n",
      "           1       0.00      0.00      0.00       118\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.35      0.50      0.41       400\n",
      "weighted avg       0.50      0.70      0.58       400\n",
      "\n",
      "3-net-InDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.83       282\n",
      "           1       0.00      0.00      0.00       118\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.35      0.50      0.41       400\n",
      "weighted avg       0.50      0.70      0.58       400\n",
      "\n",
      "4-net-InDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(model_name)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Assuming you have a model, dataloader and device (CPU or CUDA) defined\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m evaluate_model(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[102], line 17\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     15\u001b[0m         _, predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m         all_predictions\u001b[39m.\u001b[39mextend(predictions\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     18\u001b[0m         all_labels\u001b[39m.\u001b[39mextend(labels\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(classification_report(all_labels, all_predictions))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # don't calculate gradients\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "for dataset_folder in dataset_folders:\n",
    "    model_name = f\"1-net-{dataset_folder}\"\n",
    "        \n",
    "    state_dict = torch.load(f\"models/{exp_name}/{model_name}.pt\")\n",
    "    model = Net(1).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    print(model_name)\n",
    "\n",
    "    # Assuming you have a model, dataloader and device (CPU or CUDA) defined\n",
    "    evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
